[{"path":"https://www.ahl27.com/OtherTutorials/articles/BuildingTrees.html","id":"distance-matrices","dir":"Articles","previous_headings":"","what":"Distance Matrices","title":"Building Trees","text":"phylogenetic reconstruction algorithms rely upon constructing distance matrices. variety way , purposes illustration ’ll just using Hamming Distance, one simplest methods. complex methods can use nucleotide amino acid substitution models. Let’s construct toy dataset purposes illustration:  pair sequences, count proportion residues match mark distance . example calculating distance, consider first two sequences set: \\[\\begin{align*} &1)\\;\\;&AGACT&\\\\ &2)\\;\\;&AGACG& \\end{align*}\\] Since sequences differ 1 5 total residues, distance \\(\\frac{1}{5}=0.2\\). can quickly calculate complete distance matrix sequences following: Notice distance 1 2 0.2, just calculated . Now can begin create trees.","code":"sequenceSet <- DNAStringSet(c('AGACT',                               'AGACG',                               'TCATT',                               'TGCTG',                               'AGCTG')) names(sequenceSet) <- 1:5 dm <- DistanceMatrix(sequenceSet, type='dist', verbose=F) dm ##     1   2   3   4   5 ## 1 0.0 0.2 0.6 0.8 0.6 ## 2 0.2 0.0 0.8 0.6 0.4 ## 3 0.6 0.8 0.0 0.6 0.8 ## 4 0.8 0.6 0.6 0.0 0.2 ## 5 0.6 0.4 0.8 0.2 0.0"},{"path":[]},{"path":"https://www.ahl27.com/OtherTutorials/articles/BuildingTrees.html","id":"upgma","dir":"Articles","previous_headings":"Ultrametric Trees","what":"UPGMA","title":"Building Trees","text":"simplest tree building method Unweighted Pair Group Method Arithmetic Means (UPGMA). hierarchical clustering algorithm sequentially builds tree bottom . algorithm begins distance matrix, proceeds following steps: Identify pair nodes lowest distance \\(d_{min}\\). ’ve multiplied distances 10 make easier read (doesn’t make difference final result). Create new node \\(N_{new}\\) joins pairs nodes, create branch lengths leaf equidistant \\(N_{new}\\) total branch length \\(N_{new}\\) leaf half \\(d_{min}\\). case, \\(d_{min} = 2\\) new node called \\(u\\). Combine two rows distance matrix one row replacing pair entries average. average weighted number leaves pair combined, two rows clusters multiple leaves, average weighted accordingly.  Repeat steps nodes combined distance matrix single row column.    Note UPGMA methods always create ultrametric tree, means tree rooted leaves equidistant root. implies molecular clock (rate mutation across lineages tree).","code":""},{"path":"https://www.ahl27.com/OtherTutorials/articles/BuildingTrees.html","id":"wpgma","dir":"Articles","previous_headings":"Ultrametric Trees","what":"WPGMA","title":"Building Trees","text":"variation UPGMA WPGMA, W stands ‘weighted’. difference UPGMA WPGMA bit counter-intuitive. Recall combine two rows distance matrix UPGMA, weight average number leaves node combined. WPGMA, weight number leaves node, instead treat node equal weight. example, consider following distance matrix: \\[ \\begin{matrix}     & n_1 & n_2 & n_3 \\\\ n_1 & 0   & 3   & 5  \\\\ n_2 & -   & 0   & 3  \\\\ n_3 & -   & -   & 0  \\\\ \\end{matrix}\\] Now suppose node \\(n_1\\) 10 leaves, node \\(n_2\\) 2 leaves, node \\(n_3\\) 1 leaf. wanted combine nodes \\(n_1\\) \\(n_2\\) node \\(u\\) using UPGMA, distance \\(u\\) \\(n_3\\) calculated following formula: \\[\\begin{align*} d_{UPGMA}(u, n_3) &= \\frac{1}{|n_1| + |n_2|} \\left(|n_1|*d(n_1,n_3) + |n_2|*d(n_2,n_3) \\right)\\\\ \\\\ &= \\frac{1}{10+2} \\left(10*5 + 2*3 \\right) \\\\ \\\\ &= 4.67 \\end{align*}\\] However, combined WPGMA, instead ignore number leaves cluster just take simple arithmetic average, follows: \\[\\begin{align*} d_{WPGMA}(u, n_3) &= \\frac{1}{2} \\left(d(n_1,n_3) + d(n_2,n_3) \\right)\\\\ \\\\ &= \\frac{1}{2} \\left(5 + 3 \\right) \\\\ \\\\ &= 4 \\end{align*}\\] ’re probably wondering, “weighted PGMA use unweighted combinations vice-versa?” answer lies ’re referring talk pair combinations (un)weighted. Since UPGMA weights node number leaves, ’s correcting nodes representing clades difference sizes. words, combination inherently weighted, UPGMA uses clever way “undo” combination. WPGMA, contrast, weight clade size, thus combinations biased (weighted) number leaves nodes.","code":""},{"path":"https://www.ahl27.com/OtherTutorials/articles/BuildingTrees.html","id":"r-implementation","dir":"Articles","previous_headings":"Ultrametric Trees","what":"R Implementation","title":"Building Trees","text":"can create UPGMA tree R commands. Note R doubles lengths obtained , ’m going multiply distance matrix 5 get scale factor example worked .  Implementing WPGMA trees similar UPGMA trees (gives result example):  also methods combining distance matrices–see help page hclust examples.","code":"# UPGMA Tree dend <- as.dendrogram(hclust(dm*5, method='average')) plot(dend) # WPGMA Tree dend <- as.dendrogram(hclust(dm*5, method='mcquitty')) plot(dend)"},{"path":"https://www.ahl27.com/OtherTutorials/articles/BuildingTrees.html","id":"neighbor-joining-trees","dir":"Articles","previous_headings":"","what":"Neighbor Joining Trees","title":"Building Trees","text":"Another common approach building trees Neighbor-Joining (NJ) method. method also utilizes distance matrices, proceeds top-rather UPGMA’s bottom-approach. ’ll use distance matrix (multiplied 10 simpler visualization).","code":""},{"path":"https://www.ahl27.com/OtherTutorials/articles/BuildingTrees.html","id":"algorithm","dir":"Articles","previous_headings":"Neighbor Joining Trees","what":"Algorithm","title":"Building Trees","text":"algorithm proceeds following steps: Start star tree (connected single root node), record row column sums row/column matrix.  Create Q-matrix, matrix dimensions distance matrix, element given following formula: \\[Q[,j] = (n-2)d(,j) - R() - C(j)\\]       \\(n\\) number nodes consideration (number       rows distance matrix), \\(d(,j)\\) distance node \\(\\)       node \\(j\\), \\(R()\\) sum \\(\\)’th row, \\(C(j)\\) sum       \\(j\\)’th column. Identify smallest pair \\(f,g\\) Q-matrix, combine corresponding nodes adding intermediate node \\(N_{int}\\) tree.  Calculate edge lengths \\(N_{int}\\) \\(f,g\\) following formula: \\[\\begin{align*} d(N_{int}, f) &= \\frac{1}{2}(d[,j]) + \\frac{1}{2(n-2)}(R() - C(j))\\\\ \\\\ d(N_{int}, g) &= d[,j] - d[N_{int}, f] \\end{align*}\\]       case, \\(d[1,2] = 2\\), \\(R(1) = 22\\), \\(C(2) = 20\\). Thus, new node \\(u\\), : Calculate distance \\(N_{int}\\) nodes \\(h\\) following formula: \\[d(h, N_{int}) = \\frac{1}{2}(d[h,g] + d[h,f] - d[f,g])\\] Repeat full tree constructed.   three nodes left, just need figure branch lengths.  can exact way .","code":""},{"path":"https://www.ahl27.com/OtherTutorials/articles/BuildingTrees.html","id":"r-implementation-1","dir":"Articles","previous_headings":"Neighbor Joining Trees","what":"R Implementation","title":"Building Trees","text":"Neighbor-Joining trees can created R following commands. Note output slightly different due different method calculating initial distance matrix.","code":"dend <- TreeLine(myDistMatrix=dm, method='NJ') plot_tree_unrooted(dend, 'Neighbor Joining')"},{"path":"https://www.ahl27.com/OtherTutorials/articles/BuildingTrees.html","id":"maximum-parsimony-trees","dir":"Articles","previous_headings":"","what":"Maximum Parsimony Trees","title":"Building Trees","text":"last way construct phylogenies discuss maximum parsimony. method relies assumption simplest method likely best, thus tries find tree minimizes number changes tree. tree called “parsimonious” tree.","code":""},{"path":"https://www.ahl27.com/OtherTutorials/articles/BuildingTrees.html","id":"building-initial-tree","dir":"Articles","previous_headings":"Maximum Parsimony Trees","what":"Building Initial Tree","title":"Building Trees","text":"Let’s start constructing simple tree first two sequences. intermediate node labeled consensus sequence, case either sequence 1 sequence 2. equally parsimonious, since minimize total number transitions branch. ’ve labelled pink state node, orange number transitions go nodes connected edge.  ’ll now add subsequent sequence. Let’s move sequence 3. 3 total ways add sequence 3 tree:  Two trees place sequence 3 closer sequence 1/2 sequence 2/1 (resp.), middle tree places sequences equally far apart. Let’s now label trees intermediate states see parsimonious :  Short digression find intermediate/ancestral sequences: middle tree, one choice ancestral sequence, one position seen majority sequences. position (\\(AGACT\\)) appears \\(2/3\\) nodes connected (first position 1 2 \\(\\), second 1 2 \\(G\\), etc.). position nodes different, free pick bases (since equally represented thus equally parsimonious). often multiple ways construct ancestral states, end equal. Look Fitch Parsimony /Sankoff Parsimony rigorous definition find parsimonious ancestor states. Back tutorial–star tree parsimonious! tree now looks like : Just really drive point home, let’s walk adding sequence 4 tree. now 4 options can put sequence: filling ancestral states finding transitions branch:  ’m going skip walking adding sequence 5 since begins lot possibilities, suppose arrive following tree adding sequence 5:  Now initial tree, total parsimony cost 7. However, ’re done yet. Adding branches sequentially doesn’t guarantee get optimal solution, especially incorporate optimizations add branches. Many algorithms commonly minimize parsimony cost branch addition (rather recalculating entire tree) speed calculations. can cause us miss better solutions due local maxima. One common solution perturb initial tree.","code":""},{"path":"https://www.ahl27.com/OtherTutorials/articles/BuildingTrees.html","id":"nearest-neighbor-interchanges","dir":"Articles","previous_headings":"Maximum Parsimony Trees","what":"Nearest Neighbor Interchanges","title":"Building Trees","text":"common way perturb phylogenetic tree Nearest Neighbor Interchanges (NNIs). particular set 4 nodes, 3 distinct ways partition pairs. NNI changes partition rechecks parsimony result see can get improvement. NNIs significantly easier explain images. Suppose set nodes \\(\\{, b, c, d\\}\\) \\(,b\\) separated \\(c,d\\). possible NNIs look like :  Let’s explore initial tree making NNI edge separates \\(\\{4,5\\}\\) rest nodes. ’ll two trees check:  mentioned , total cost current tree 7. new trees better?  tree 2 worse, tree 1 just good initial tree! ’m going skip walking NNIs branches, let’s look final two equally parsimonious reconstructions (node labels blue, branch lengths orange):  Note can remove branches length 0.","code":""},{"path":"https://www.ahl27.com/OtherTutorials/articles/BuildingTrees.html","id":"r-implementation-2","dir":"Articles","previous_headings":"Maximum Parsimony Trees","what":"R Implementation","title":"Building Trees","text":"can maximum parsimony reconstructions R using following code:  Notice almost exactly matches one reconstructions! Parsimony reconstructions can differ based calculate parsimony, can account lengths plot slightly different . Using different transition cost model (weighting transitions different transversions) can lead different results.","code":"dend <- TreeLine(sequenceSet, method='MP') plot_tree_unrooted(dend, 'Maximum Parsimony')"},{"path":"https://www.ahl27.com/OtherTutorials/articles/ComparingTrees.html","id":"setup","dir":"Articles","previous_headings":"","what":"Setup","title":"Comparing Trees","text":"’ll start slightly larger test dataset can clearly see differences tree constructions. following codeblocks read set 25 simulated alignments, constructs phylogenetic trees using three methods discussed previously.     Visualizing trees ’ve made:","code":"# External data file, contains simulated alignment simSeqsFile <- system.file('extdata', 'Simulated_v1.fas', package='LakshmanTutorials', mustWork=TRUE)  simAli <- suppressWarnings(readDNAStringSet(simSeqsFile))[1:25] names(simAli) <- 1:25 simDm <- DistanceMatrix(simAli, type='dist', verbose=FALSE)  UPGMAtree <- as.dendrogram(hclust(simDm, method='average')) NJtree <- TreeLine(simAli, myDistMatrix=simDm, method='NJ') MPtree <- TreeLine(simAli, myDistMatrix=simDm, method='MP') plot_tree_unrooted(UPGMAtree, 'UPGMA') plot_tree_unrooted(NJtree, 'NJ') plot_tree_unrooted(MPtree, 'MP')"},{"path":"https://www.ahl27.com/OtherTutorials/articles/ComparingTrees.html","id":"partitions","dir":"Articles","previous_headings":"","what":"Partitions","title":"Comparing Trees","text":"dive tree distances, ’s important understand concept partitions. bifurcating unrooted tree, every edge divides set leaf nodes two sets. form basis many tree comparison algorithms. Let’s look toy example:  Note numbered edge tree splits labeled leaf nodes two distinct groups either side . Leaf edges trivially split tree partition just leaf everything else, internal edges split leaves interesting partitions.","code":""},{"path":"https://www.ahl27.com/OtherTutorials/articles/ComparingTrees.html","id":"rf-distance","dir":"Articles","previous_headings":"","what":"RF Distance","title":"Comparing Trees","text":"Robinson-Foulds (RF) distance measures similarity partitions tree. Let’s look toy example two small trees internal partitions labeled (internal edges circled green):  Now let \\(\\) number partitions unique first tree, \\(B\\) number partitions unique second tree. RF distance (also called symmetric difference metric) simply quantity \\((+B)\\). example, note edge 2 identical partition edge II. Thus, first tree one unique partition, second tree one unique partition, RF distance \\(1+1=2\\). implementations change metric slightly scaling , either dividing two scaling metric maximum value 1. latter operation can done dividing maximum possible score, just sum total number branches. case, total number internal branches 4 (2 tree), RF distance \\(0.5\\).","code":""},{"path":"https://www.ahl27.com/OtherTutorials/articles/ComparingTrees.html","id":"r-implementation","dir":"Articles","previous_headings":"RF Distance","what":"R implementation","title":"Comparing Trees","text":"","code":"RF_dist_external <- function(dend1, dend2){   tf <- tempfile()   WriteDendrogram(dend1, file=tf, quoteLabels=FALSE)   predTree1 <- read.tree(tf)      tf <- tempfile()   WriteDendrogram(dend2, file=tf, quoteLabels=FALSE)   predTree2 <- read.tree(tf)      return(dist.topo(predTree1, predTree2, 'PH85')) }  RF_dist_external(UPGMAtree, NJtree) ##       tree1 ## tree2    12 RF_dist_external(UPGMAtree, MPtree) ##       tree1 ## tree2    18 RF_dist_external(NJtree, MPtree) ##       tree1 ## tree2    12"},{"path":"https://www.ahl27.com/OtherTutorials/articles/ComparingTrees.html","id":"drawbacks","dir":"Articles","previous_headings":"RF Distance","what":"Drawbacks","title":"Comparing Trees","text":"RF distance widely used, common issues kept mind using : Similar trees can receive value Range values depends tree shape Doesn’t look branch length Can’t recognize similar clades (difference increases distance score) Original implementation assumes bifurcating, unrooted tree leaf set Distance immediately statistically interpretable (larger \\(\\neq\\) significant) drawbacks accounted subsequent optimization, ‘generalized RF distances’ created can account similar sets working multifurcating trees different leaf sets.","code":""},{"path":"https://www.ahl27.com/OtherTutorials/articles/ComparingTrees.html","id":"kf-distance","dir":"Articles","previous_headings":"","what":"KF Distance","title":"Comparing Trees","text":"Kuhner-Felsenstein (KF) distance attempts incorporate branch lengths RF distance gain richer description differences two trees. Recall RF distance number unique (non-shared) partitions tree. KF distance instead taken sum squared difference branch lengths equivalent partition two trees. partition unique particular tree, taken branch length 0 tree. updated version previous example branch lengths added pink (partitions labeled):  also examine leaf branches, since leaf set branch lengths identical, branches cancel . One pair branches equivalent partition (2 II), two unique. KF distance : advantages RF distance can incorporate branch lengths addition topology, allows downweight small differences topology upweight large differences.","code":""},{"path":"https://www.ahl27.com/OtherTutorials/articles/ComparingTrees.html","id":"r-implementation-1","dir":"Articles","previous_headings":"KF Distance","what":"R Implementation","title":"Comparing Trees","text":"","code":"KF_dist_external <- function(dend1, dend2){   tf <- tempfile()   WriteDendrogram(dend1, file=tf, quoteLabels=FALSE)   predTree1 <- read.tree(tf)      tf <- tempfile()   WriteDendrogram(dend2, file=tf, quoteLabels=FALSE)   predTree2 <- read.tree(tf)      return(dist.topo(predTree1, predTree2, method='score')) }  RF_dist_external(UPGMAtree, NJtree) ##       tree1 ## tree2    12 RF_dist_external(UPGMAtree, MPtree) ##       tree1 ## tree2    18 RF_dist_external(NJtree, MPtree) ##       tree1 ## tree2    12"},{"path":"https://www.ahl27.com/OtherTutorials/articles/ComparingTrees.html","id":"generalized-robinson-foulds-distance","dir":"Articles","previous_headings":"","what":"Generalized Robinson-Foulds Distance","title":"Comparing Trees","text":"mentioned , RF distance number drawbacks. Correcting led family metrics referred “Generalized Robinson-Foulds Distances”. metrics attempt measure differences partitions binary values, continuous measures incorporate different given partition another. Two successful measures information-theoretic generalized RF Distance measures, Phylogenetic Information Content Mutual Clustering Information. Description metrics sourced Smith (2020). RF Distance always operates partitions. Let given partition defined \\(S = |B\\), \\(\\) \\(B\\) disjoint leaf sets. classic RF Distance, \\(S\\) identical \\(S\\) exists tree, add one distance.","code":""},{"path":"https://www.ahl27.com/OtherTutorials/articles/ComparingTrees.html","id":"phylogenetic-information-content","dir":"Articles","previous_headings":"Generalized Robinson-Foulds Distance","what":"Phylogenetic Information Content","title":"Comparing Trees","text":"Phylogenetic Information Content, instead score pair partitions using probability encounter similar partition chance. given set \\(n\\) genomes, \\((2n-5)!!\\) possible unrooted binary trees. \\(x!!\\) double factorial, defined \\(x!! = x * (x-2)!!\\) \\(1!!=0!!=1\\). given pair splits \\(S_1 = A_1|B_1\\) \\(S_2 = A_2|B_2\\) tree \\(n\\) leaves, probability randomly chosen binary tree containing two splits : obtain shared phylogenetic information, pair splits set \\(h = 0\\) \\(S_1\\) \\(S_2\\) conflict, \\(h(S_1) + h(S_2) + h(S_1,S_2)\\) \\(h(S_1,S_2) = -\\log(P(S_1,S_2))\\). Summing value across optimal pairing nodes results Shared Phylogenetic Information Score.","code":""},{"path":"https://www.ahl27.com/OtherTutorials/articles/ComparingTrees.html","id":"mutual-clustering-information","dir":"Articles","previous_headings":"Generalized Robinson-Foulds Distance","what":"Mutual Clustering Information","title":"Comparing Trees","text":"Mutual Clustering Information similar Shared Phylogenetic Information quantifies similarity given partition, metric instead uses mutual information partition rather p-value-like measure. Let \\(P() = \\frac{||}{n}\\), \\(P(A_1,A_2) = \\frac{|A_1 \\cap A_2|}{n}\\). mutual clustering information two splits \\(S_1 = A_1|B_1\\) \\(S_2 = A_2|B_2\\) given : individual entropy given split \\(h(S)\\) given \\(-[P()\\log P() +P(B)\\log P(B)]\\). Summing \\(\\) optimal pairing splits produces mutual clustering information score. can converted distance taking: \\(S_i \\T_i\\) splits first tree \\(Pairing\\) optimal pairing splits two trees.","code":""},{"path":"https://www.ahl27.com/OtherTutorials/articles/ComparingTrees.html","id":"r-implementation-2","dir":"Articles","previous_headings":"Generalized Robinson-Foulds Distance","what":"R Implementation","title":"Comparing Trees","text":"","code":"library(SynExtend)  GeneralizedRF(UPGMAtree, NJtree) GeneralizedRF(UPGMAtree, MPtree) GeneralizedRF(NJtree, MPtree)"},{"path":"https://www.ahl27.com/OtherTutorials/articles/ComparingTrees.html","id":"other-metrics","dir":"Articles","previous_headings":"","what":"Other Metrics","title":"Comparing Trees","text":"two commonly used metrics, many implemented TreeDist package. Notable tree distance measures include: (Böcker et al., 2013) (Steel Penny, 1993) (MAST: Kao et al., 2001; MASTI: Smith 2020a). (Hein 1990) (Estabrook et al., 1985) (Li et al., 1996) comprehensive evaluation many metrics available paper.","code":""},{"path":"https://www.ahl27.com/OtherTutorials/articles/ComparingTrees.html","id":"manual-implementations","dir":"Articles","previous_headings":"","what":"Manual Implementations","title":"Comparing Trees","text":"wrote implementations RF, KF, GRF distances scratch illustrate functions working hood. Note external packages incorporate optimizations RF/KF didn’t implement lead different results. best GeneralizedRF(), available SynExtend (shown since significantly longer two methods).","code":""},{"path":"https://www.ahl27.com/OtherTutorials/articles/ComparingTrees.html","id":"helper-functions","dir":"Articles","previous_headings":"Manual Implementations","what":"Helper functions","title":"Comparing Trees","text":"code block contains several helper functions used later.","code":"flatdendrapply <- function(dend, NODEFUN, LEAFFUN=NODEFUN, INCLUDEROOT=TRUE, ...){   ## Applies a function to each node (internal and leaf) of the tree   ## Returns a flat list   val <- lapply(dend,                  \\(x){                   if (is.null(attr(x, 'leaf'))){                     v <- list(NODEFUN(x, ...))                     for ( child in x ) v <- c(v, Recall(child))                     return(v)                   }                    else if (!is(LEAFFUN, 'function'))                     return(list())                   else                      return(list(LEAFFUN(x, ...)))                 }   )   retval <- unlist(val, recursive=FALSE)   if (!INCLUDEROOT)     retval[[1]] <- NULL    return(retval) }  isLeaf <- function(dendNode){   return(!is.null(attr(dendNode, 'leaf')) && attr(dendNode, 'leaf')) }  equivPart <- function(set1, set2, fullset){   # Checks if two partitions are equivalent   inverseset1 <- fullset[!(fullset %in% set1)]   return(setequal(set1,set2) || setequal(inverseset1, set2)) }  get_branch_length <- function(dendNode){   ## Helper function for KF distance, gets partition and branch length   ## of all branches Because of weirdness each node will return two values,   ## the result just needs some slight post-processing    ## (see KF_Distance function for example)   if(isLeaf(dendNode)){     return(0)   }      h <- attr(dendNode, 'height')   n1 <- dendNode[[1]]   n2 <- dendNode[[2]]   c1 <- attr(n1, 'height')   c2 <- attr(n2, 'height')      if(isLeaf(n1))     labs1 <- attr(n1, 'label')   else      labs1 <- unlist(n1)      if (isLeaf(n2))     labs2 <- attr(n2, 'label')   else     labs2 <- unlist(n2)    l1 <- list(length=h-c1, part=labs1)   l2 <- list(length=h-c2, part=labs2)   return(list(l1, l2)) }"},{"path":"https://www.ahl27.com/OtherTutorials/articles/ComparingTrees.html","id":"robinson-foulds-distance","dir":"Articles","previous_headings":"Manual Implementations","what":"Robinson-Foulds Distance","title":"Comparing Trees","text":"","code":"RF_Distance <- function(dend1, dend2){   # Get all partitions   part1 <- flatdendrapply(dend1, unlist, NULL)   part2 <- flatdendrapply(dend2, unlist, NULL)   allmembers <- unique(c(unlist(dend1), unlist(dend2)))      # Calculate tree distance   A <- B <- 0   for ( i in seq_along(part1))     A <- A + !any(sapply(part2, \\(x) equivPart(part1[[i]], x, allmembers)))      for ( i in seq_along(part2))     B <- B + !any(sapply(part1, \\(x) equivPart(part2[[i]], x, allmembers)))      # This implementation normalizes to get a distance out of 1   return((A+B) / (length(part1) + length(part2))) }"},{"path":"https://www.ahl27.com/OtherTutorials/articles/ComparingTrees.html","id":"kuhner-felsenstein-distance","dir":"Articles","previous_headings":"Manual Implementations","what":"Kuhner-Felsenstein Distance","title":"Comparing Trees","text":"","code":"KF_Distance <- function(dend1, dend2){   # Get all branch lengths and partitions   part1 <- flatdendrapply(dend1, get_branch_length, NULL)   part2 <- flatdendrapply(dend2, get_branch_length, NULL)      # Each function call returns a length of list two, we just want the members   part1 <- unlist(part1, recursive=FALSE)   part2 <- unlist(part2, recursive=FALSE)      # Root is split into two branches, need to combine   part1[[1]]$length <- part1[[1]]$length + part1[[2]]$length   part2[[1]]$length <- part2[[1]]$length + part2[[2]]$length   part1[[2]] <- part2[[2]] <- NULL      allmembers <- unique(c(unlist(dend1), unlist(dend2)))      # For each   treedist <- 0   for ( i in seq_along(part1)){     check <- sapply(part2, \\(x) equivPart(part1[[i]]$part, x$part, allmembers))     if (any(check)){       loc <- which(check)       treedist <- treedist + (part1[[i]]$length - part2[[loc]]$length)**2     }   }   for ( i in seq_along(part2)){     check <- sapply(part1, \\(x) equivPart(part2[[i]]$part, x$part, allmembers))     if (any(check)){       loc <- which(check)       treedist <- treedist + (part2[[i]]$length - part1[[loc]]$length)**2     }   }      ## divided by two since duplicates will be added twice   ## probably worth reworking at some point to avoid adding duplicates twice,   ## this is just a quick fix   return(sqrt(treedist/2)) }"},{"path":"https://www.ahl27.com/OtherTutorials/articles/ComparingTrees.html","id":"generalized-rf-distance","dir":"Articles","previous_headings":"Manual Implementations","what":"Generalized RF Distance","title":"Comparing Trees","text":"","code":"GRF <- function(dend1, dend2){   require(SynExtend)   return(GeneralizedRF(dend1, dend2)) }"},{"path":"https://www.ahl27.com/OtherTutorials/articles/ComparingTrees.html","id":"comparison","dir":"Articles","previous_headings":"Manual Implementations","what":"Comparison","title":"Comparing Trees","text":"","code":"RFDists <- KFDists <- GRFDists <- matrix(0, nrow=3, ncol=3) rownames(RFDists) <- rownames(KFDists) <- rownames(GRFDists) <-   colnames(RFDists) <- colnames(KFDists) <- colnames(GRFDists) <-   c('UPGMA', 'NJ', 'MP')  RFDists[1,2] <- RFDists[2,1] <- RF_Distance(UPGMAtree, NJtree) RFDists[1,3] <- RFDists[3,1] <- RF_Distance(UPGMAtree, MPtree) RFDists[2,3] <- RFDists[3,2] <- RF_Distance(NJtree, MPtree) KFDists[1,2] <- KFDists[2,1] <- KF_Distance(UPGMAtree, NJtree) KFDists[1,3] <- KFDists[3,1] <- KF_Distance(UPGMAtree, MPtree) KFDists[2,3] <- KFDists[3,2] <- KF_Distance(NJtree, MPtree) #GRFDists[1,2] <- GRFDists[2,1] <- GRF(UPGMAtree, NJtree) #GRFDists[1,3] <- GRFDists[3,1] <- GRF(UPGMAtree, MPtree) #GRFDists[2,3] <- GRFDists[3,2] <- GRF(NJtree, MPtree)  RFDists ##           UPGMA        NJ        MP ## UPGMA 0.0000000 0.2608696 0.3913043 ## NJ    0.2608696 0.0000000 0.2608696 ## MP    0.3913043 0.2608696 0.0000000 KFDists ##           UPGMA        NJ        MP ## UPGMA 0.0000000 0.9712953 1.0367032 ## NJ    0.9712953 0.0000000 0.3419947 ## MP    1.0367032 0.3419947 0.0000000 #GRFDists"},{"path":"https://www.ahl27.com/OtherTutorials/articles/MultipleTesting.html","id":"types-of-error","dir":"Articles","previous_headings":"","what":"Types of Error","title":"Multiple Testing Correction","text":"talk correction procedures, first need talk “error” means. single statistical tests, ’re typically worried Type Type II error. Type error incorrectly reject null hypothesis. observed striking result due chance, leading false positive. single statistical tests, probability Type error significance level (\\(\\alpha=0.05\\), ’s 5%). Type II error inverse–fail reject null hypothesis rejected. normally extreme process happens produce normal looking result chance, leading false negative. single statistical tests, probability Type II error called \\(\\beta\\). power statistical test given \\(1-\\beta\\).  move multiple testing, instead examine Family-Wise Error Rate (FWER) False Discovery Rate (FDR). Multiple testing correction often trade-minimizing FWER FDR. FWER probability making least one Type error within group tests. Suppose \\(n\\) tests–individual test Type error rate \\(\\alpha\\) tests independent, FWER (\\(\\bar \\alpha\\)) simply: \\[ \\bar \\alpha = 1 - (1-\\alpha)^n\\] FDR expected proportion Type errors across tests. Put terms true/false positives: \\[ FDR = \\frac{FP}{FP+TP} \\] Corrections minimize FDR result higher statistical power cost Type errors. Minimizing FWER opposite; fewer Type errors, lower statistical power.","code":""},{"path":"https://www.ahl27.com/OtherTutorials/articles/MultipleTesting.html","id":"minimizing-fwer-for-multiple-hypotheses","dir":"Articles","previous_headings":"","what":"Minimizing FWER for Multiple Hypotheses","title":"Multiple Testing Correction","text":"first group methods deal minimizing FWER set multiple hypotheses. drug testing example earlier perfect example. Another example testing set SNPs significant association particular disease–null hypothesis \\(H_i\\) SNP \\(\\) significantly associated disease.","code":""},{"path":"https://www.ahl27.com/OtherTutorials/articles/MultipleTesting.html","id":"bonferroni-correction","dir":"Articles","previous_headings":"Minimizing FWER for Multiple Hypotheses","what":"Bonferroni Correction","title":"Multiple Testing Correction","text":"Bonferroni correction one simplest ways minimize FWER across multiple tests. motivation method comes Boole’s Inequality, implies following result \\(n\\) independent tests: \\[\\bar \\alpha \\leq n\\alpha\\] solving equation \\(\\alpha\\), see set \\(\\alpha = \\frac{\\bar \\alpha}{n}\\), can constrain FWER value ’d like. example, ensure FWER 0.1 across 100 tests, simply test significance \\(\\alpha = \\frac{0.01}{100} = 10^{-4}\\) significance level. method simple, comes drawbacks. notably, result depends fact individual tests independent–test dependent, correction conservative necessary. Bonferroni correction also less statistical power alternatives, like Holm-Bonferrroni Šidák correction.","code":""},{"path":"https://www.ahl27.com/OtherTutorials/articles/MultipleTesting.html","id":"šidák-correction","dir":"Articles","previous_headings":"Minimizing FWER for Multiple Hypotheses","what":"Šidák Correction","title":"Multiple Testing Correction","text":"Šidák correction slightly less stringent Bonferroni correction still conserving fixed FWER rate. procedure, instead back solve initial calculation FWER: \\[\\begin{align*} \\bar \\alpha &= 1 - (1-\\alpha)^n \\\\ \\implies \\alpha &= 1 - (1-\\bar \\alpha)^\\frac{1}{n} \\end{align*}\\] previous example 100 tests desired FWER 0.1, ’d test significance \\(\\alpha = 1-(1-0.1)^{0.01} = 0.00105\\) level, slightly higher Bonferroni’s \\(0.001\\) significance level. Note test also requires independence individual statistical tests.","code":""},{"path":"https://www.ahl27.com/OtherTutorials/articles/MultipleTesting.html","id":"holm-bonferroni","dir":"Articles","previous_headings":"Minimizing FWER for Multiple Hypotheses","what":"Holm-Bonferroni","title":"Multiple Testing Correction","text":"Holm-Bonferroni improvement Bonferroni correction uniformly powerful, meaning always least powerful Bonferroni. method employs sorting p-values set null hypotheses. Holm-Bonferroni correction, first sort p-values \\(p_1,\\dots,p_n\\) \\(p_i \\leq p_{+1} \\;\\forall \\). also sort null hypotheses correspondingly \\(H_1,\\dots,H_n\\). , \\(p_k\\), test \\(p_k < \\frac{\\bar \\alpha}{n+1-k}\\). , reject \\(H_k\\) continue \\(p_{k+1}\\). , stop reject hypotheses. Holm-Bonferroni notably require statistical test independent; functions correctly dependence structure p-values.","code":""},{"path":"https://www.ahl27.com/OtherTutorials/articles/MultipleTesting.html","id":"hochberg-step-up","dir":"Articles","previous_headings":"Minimizing FWER for Multiple Hypotheses","what":"Hochberg Step-Up","title":"Multiple Testing Correction","text":"subtly different test Hochberg step-procedure, uniformly powerful Holm-Bonferroni requires test statistically independent positively dependent. procedure Holm-Bonferroni, instead iteratively testing p-values, find maximal \\(k\\) \\(p_k < \\frac{\\bar \\alpha}{n+1-k}\\), reject null hypotheses \\(H_1,\\dots,H_k\\).","code":""},{"path":"https://www.ahl27.com/OtherTutorials/articles/MultipleTesting.html","id":"minimizing-fwer-for-one-hypothesis","dir":"Articles","previous_headings":"","what":"Minimizing FWER for One Hypothesis","title":"Multiple Testing Correction","text":"methods slightly different tests whether groups p-values statistically significant, rather examining individual p-values null hypotheses. Thus, methods ideal situations several statistical tests conducted overall null hypothesis. example kind problem testing whether new curriculum improves students’ scores standardized tests. statistical test looks paired experiment control classrooms, investigates performance experimental group significantly different control. null hypotheses groups, new curriculum significantly impact outcomes. methods test whether groups p-values statistically significant, rather examining individual scores. methods determine overall findings significant, previous section determine specific tests significant.","code":""},{"path":"https://www.ahl27.com/OtherTutorials/articles/MultipleTesting.html","id":"fishers-method","dir":"Articles","previous_headings":"Minimizing FWER for One Hypothesis","what":"Fisher’s Method","title":"Multiple Testing Correction","text":"Fisher’s Method one simplest methods multiple testing correction single hypothesis case. method calculates test statistic : \\[ \\chi_{2n}^2 \\sim -2\\sum_{=1}^n \\log {p_i} \\] formula follows fact , independent unbiased statistical tests, distribution p-values uniformly distributed \\([0,1]\\). negative log-transform uniformly distributed variable produces exponential distribution, scaling two yields chi-squared distributions two degrees freedom. Summing \\(n\\) chi-squared distributions two degrees freedom produces chi-squared distribution \\(2n\\) degrees freedom. statistic distribution, simple calculate overall p-value set statistical tests. Note calculation depends statistical tests independent . p-values dependence, Fisher’s Method produce result anti-conservative, meaning occasionally incorrectly reject null hypothesis. Additionally, Fisher’s Method overstate evidence null hypothesis cases null correctly rejected.","code":""},{"path":"https://www.ahl27.com/OtherTutorials/articles/MultipleTesting.html","id":"browns-method","dir":"Articles","previous_headings":"Minimizing FWER for One Hypothesis","what":"Brown’s Method","title":"Multiple Testing Correction","text":"Brown’s Method improves Fisher’s Method cases p-values independent covariance structure data \\(X\\) known. case, can replace \\(\\chi^2(n)\\) distribution scaled chi-squared distribution \\(c\\chi^2(n')\\), new constants calculated : \\[\\begin{align*} c &= \\frac{Var(X)}{2E[X]}\\\\&\\\\ n' &= \\frac{2\\left(E[X]\\right)^2}{Var(X)} \\end{align*}\\] often best approximation data generated multivariate normal distribution known covariance, fairly specific situation.","code":""},{"path":"https://www.ahl27.com/OtherTutorials/articles/MultipleTesting.html","id":"cauchy-combination-test","dir":"Articles","previous_headings":"Minimizing FWER for One Hypothesis","what":"Cauchy Combination Test","title":"Multiple Testing Correction","text":"Instead transforming p-values \\(\\chi^2\\) distribution, tangent transformation can transform data distribution tail approximately Cauchy distribution allowing arbitrary dependence p-values. test statistic given : \\[X = \\sum_{=1}^n \\omega_i \\tan\\left[(0.5-p_i)\\pi\\right]\\] \\(\\omega_i\\) weights sum one. Given standard Cauchy random variable W, \\[\\lim_{t\\\\infty} \\frac{P[X>t]}{P[W>t]} = 1\\] Thus, can find p-value tests together comparing \\(X\\) quantiles standard Cauchy distribution.","code":""},{"path":"https://www.ahl27.com/OtherTutorials/articles/MultipleTesting.html","id":"harmonic-mean-p-value","dir":"Articles","previous_headings":"Minimizing FWER for One Hypothesis","what":"Harmonic Mean P-Value","title":"Multiple Testing Correction","text":"p-values independent variance data unknown, harmonic mean p-value (HMP) often best solution. Given p-values tests, HMP calculated : \\[\\dot p = \\frac{\\sum_{=1}^n w_i}{\\sum_{=1}^nw_i/p_i}\\] \\(w_i\\) weights p-value sum weights adds one. tests equally weighted, \\(w_i=1/n \\;\\forall \\). HMP generally anti-conservative, less Fisher’s Method. level anti-conservativeness decreases \\(\\dot p\\), low values HMP less error. strong enough HMP directly interpretable levels \\(\\dot p \\leq 0.05\\). asymptotically exact p-value can calculated HMP using Landau distribution. Note also multilevel test can designed use HMP subsets original tests maintaining FWER. Using HMP subset \\(S\\) p-values rejecting \\(\\dot p \\leq \\alpha\\sum_{\\S} w_i\\) approximately control FWER level \\(\\alpha\\). method can used find significant subsets overall group tests. HMP number really great properties: Robust positive dependency p-values Insensitive number tests Robust distribution weights influenced smallest p-values HMP significant, neither HMPs subsets tests Controls FWER lower FDR (greater power) Benjamini-Hochberg","code":""},{"path":"https://www.ahl27.com/OtherTutorials/articles/MultipleTesting.html","id":"minimizing-fds-for-multiple-hypotheses","dir":"Articles","previous_headings":"","what":"Minimizing FDS for Multiple Hypotheses","title":"Multiple Testing Correction","text":"methods try minimize False Discovery Rate, generally resulting higher FWER also higher statistical power. Note also FDS-based measures scale data–try limit proportion overall false positives rather probability least one false positive. Thus, tests FDR yield false positives tests FWER.","code":""},{"path":"https://www.ahl27.com/OtherTutorials/articles/MultipleTesting.html","id":"benjamini-hochberg","dir":"Articles","previous_headings":"Minimizing FDS for Multiple Hypotheses","what":"Benjamini-Hochberg","title":"Multiple Testing Correction","text":"method bears resemblance Hochberg step-procedure, constrains FDR \\(\\alpha\\). Like Hochberg, first sort p-values ascending order. find largest \\(k\\) \\(p_k \\leq \\frac{k}{n}\\alpha\\). corresponding null hypotheses \\(H_1,\\dots,H_k\\) rejected, remaining null hypotheses rejected. procedure can observed geometrically first plotting \\((k,p_k)\\) p-values, drawing line \\(y=\\frac{\\alpha}{n}x\\). points line correspond hypotheses rejected, points fail reject.  green points can reject null hypothesis , red . Note green points fall line–reject p-values less greatest point falls line, even smaller points don’t fall line. example, eighth point largest less equal line, can reject hypotheses corresponding first eight points. Benjamini-Hochberg valid tests independent cases dependence, isn’t universally valid. concern dependence values, use Benjamini-Yekutieli HMP instead.","code":""},{"path":"https://www.ahl27.com/OtherTutorials/articles/MultipleTesting.html","id":"benjamini-yekutieli","dir":"Articles","previous_headings":"Minimizing FDS for Multiple Hypotheses","what":"Benjamini-Yekutieli","title":"Multiple Testing Correction","text":"Benjamini-Yekutieli improves Benjamini-Hochberg allowing good performance arbitrary dependence scenarios. algorithm , modify bound checking largest \\(k\\) \\(p_k \\leq \\frac{k}{c(n)* n}\\alpha\\). \\(c(n)\\) new parameter defined follows: tests independent positively correlated, \\(c(n) = 1\\). arbitrary dependence, \\(c(n) = \\sum_{=1}^n n^{-1}\\), \\(n^{th}\\) harmonic number. necessary, \\(n^{th}\\) harmonic number can approximated \\(c(n) \\approx \\ln(n) + 0.57721 + (2n)^{-1}\\). constant comes Euler-Mascheroni constant (truncated ). approximation can useful large values \\(n\\). harmonic number can used ’re unsure whether data dependent independent; scenario independence rejection criteria slightly conservative.","code":""},{"path":"https://www.ahl27.com/OtherTutorials/articles/MultipleTesting.html","id":"conclusions","dir":"Articles","previous_headings":"","what":"Conclusions","title":"Multiple Testing Correction","text":"multitude methods variety applications presented . following table briefly summarizes main takeaways.","code":""},{"path":"https://www.ahl27.com/OtherTutorials/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Aidan Lakshman. Author, maintainer.","code":""},{"path":"https://www.ahl27.com/OtherTutorials/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Lakshman (2023). LakshmanTutorials: LakshmanTutorials. R package version 1.0.0, https://www.ahl27.com/othertutorials/.","code":"@Manual{,   title = {LakshmanTutorials: LakshmanTutorials},   author = {Aidan Lakshman},   year = {2023},   note = {R package version 1.0.0},   url = {https://www.ahl27.com/othertutorials/}, }"},{"path":"https://www.ahl27.com/OtherTutorials/index.html","id":"welcome","dir":"","previous_headings":"","what":"Random Tutorials","title":"Random Tutorials","text":"repository serves place put miscellaneous tutorials ’ve written conferences, lab meetings, settings. far, ’ve included basic introduction common methods used phylogenetics. ’ll keep page updated new tutorials make ! jump tutorials, check “Tutorials” tab top bar. want return website, can either click click “Back Website” top bar. Please let know issues things aren’t clear tutorials! main goal tutorial useful readers, ’m always interested feedback. Feel free send email ahl27@pitt.edu, open issue GitHub Repository. Thank !","code":""},{"path":"https://www.ahl27.com/OtherTutorials/index.html","id":"useful-links","dir":"","previous_headings":"","what":"Useful Links","title":"Random Tutorials","text":"DECIPHER SynExtend Related Tutorials Lab!          work licensed Creative Commons Attribution-ShareAlike 4.0 International License.","code":""},{"path":[]},{"path":"https://www.ahl27.com/OtherTutorials/news/index.html","id":"general-notes-2-0","dir":"Changelog","previous_headings":"","what":"General Notes","title":"Version 2.0","text":"Adds new multiple testing correction tutorials Updates webpage make general future expansion","code":""},{"path":[]},{"path":"https://www.ahl27.com/OtherTutorials/news/index.html","id":"general-notes-1-1","dir":"Changelog","previous_headings":"","what":"General Notes","title":"Version 1.1","text":"Adds new section Generalized RF Distances Comparing Trees","code":""},{"path":[]},{"path":"https://www.ahl27.com/OtherTutorials/news/index.html","id":"general-notes-1-0","dir":"Changelog","previous_headings":"","what":"General Notes","title":"Version 1.0","text":"Initialization package Implementation Building Trees tutorial Implementation Comparing Trees tutorial","code":""}]
